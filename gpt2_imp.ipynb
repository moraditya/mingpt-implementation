{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first load the gpt2 (24M from openai gpt2 paper) and then use the setting of weights that is the 124M model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use attention is all you need paper and notice how gpt2 transformer is a decoder only model (cross attention that uses the encoder is also missing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  match up the hugging face transformers scheme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "loading weights from pretrained gpt: gpt2\n",
      "Generated 0/200 tokens\n",
      "Generated 20/200 tokens\n",
      "Generated 40/200 tokens\n",
      "Generated 60/200 tokens\n",
      "Generated 80/200 tokens\n",
      "Generated 100/200 tokens\n",
      "Generated 120/200 tokens\n",
      "Generated 140/200 tokens\n",
      "Generated 160/200 tokens\n",
      "Generated 180/200 tokens\n",
      "\n",
      "==== GENERATED STORY ====\n",
      "\n",
      "Hello, I'm a large language model, but I don't know quite what to do with it.\n",
      "\n",
      "I don't know any of the languages that the designers use. Do you think it's still possible to have several languages in one interface?\n",
      "\n",
      "It's possible. It's very likely that there will still be some languages that will be very similar to what we do now without being confused with the languages that are still used in the past.\n",
      "\n",
      "People who have this habit of building new ones are not necessarily great people. If there were a question about the future of languages, would a question like \"What are the future languages going to be?\" be pretty easy?\n",
      "\n",
      "I think that there's a lot of possibility that there will be some language-systems that will be more like what we have now. I believe that the future will be more like what we've already known. I think that there are languages like Java that will be very similar to what we know today, but with more features.\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "import math \n",
    "import inspect\n",
    "master_process = True\n",
    "# lets first build the SKELETON!!!!\n",
    "# --------------------------------------------------------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    # tokens are lined up (1024), each token emits three vectors: query, key,value\n",
    "    # queries and keys have to multiply to get the \"amount\" of attention\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "            .view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "\n",
    "        # autoregressive masks only attend to tokens before them \n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "\n",
    "        # normalizes them\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # attention matrix multiplied by values to get a weighted sum of the tokens\n",
    "        # that we found interesting\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # concatenate the tokens\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # The MLP consists of two linear projections, with a GELU nonlinearity in between.\n",
    "    # GELU (Gaussian Error Linear Unit) is an activation function that smooths out \n",
    "    # ReLU-like behavior by using a probabilistic approach based on the Gaussian distribution.\n",
    "    # Unlike ReLU, which applies a hard threshold, GELU allows small negative values to pass \n",
    "    # through, improving gradient flow and stability.\n",
    "    #\n",
    "    # GPT-2 uses GELU instead of ReLU because it enables better convergence and \n",
    "    # smoother learning dynamics, especially in deep architectures, which would\n",
    "    # not occur with a RElU because of the dead neuron problem. The difference\n",
    "    # lies in the tail of a RELU, all activations have no gradient. In GELU, \n",
    "    # there are local gradients, which means that dynamic learning ensues.\n",
    "    # This is why you want to adopt the nonlinearity. \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc       = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu       = nn.GELU(approximate='tanh')  # Tanh approximation speeds up computation\n",
    "        self.c_proj     = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    # Unlike the original Transformer architecture from \"Attention Is All You Need\",\n",
    "    # GPT-2 places LayerNorm *before* the attention and MLP layers instead of inside them.\n",
    "    # This ensures a clean residual pathway for gradients, improving stability during training.\n",
    "    # In contrast, the original Transformer applied LayerNorm *after* the residual connections,\n",
    "    # which introduced dependencies that could make training less stable.\n",
    "    \n",
    "    # Gradients represent the direction and magnitude of change needed to minimize \n",
    "    # the loss function during backpropagation. A clean residual pathway means gradients \n",
    "    # can flow more smoothly through the network, reducing issues like vanishing or exploding gradients.\n",
    "\n",
    "    # Attention is a communication operation - all 1024 tokens that are lined up\n",
    "    # and communicate, where they exchange information. MLP, however, happens \n",
    "    # individually - or in other words, no information is exchanged between the \n",
    "    # tokens. So self.attn is the reduce and self.mlp is the MAP! You basically\n",
    "    # communicate first then think individually about the gathered information.\n",
    "    # Every transformer block, then, iteratively refines the representations\n",
    "    # of the residual streams\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x \n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\n",
    "    # set the hyperparameters to match the gpt2 124M model\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12 \n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "# try to match the hugging face transformers/gpt2 schema (which we saw earlier)\n",
    "# to do this, we will reflect the transformer container from play.ipynb \n",
    "# by using nn.ModuleDict - this allows you to index into the submodules using keys\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # weights of the token (wte) and position (wpe) embedding \n",
    "            # (nn embedding is just a fancy wrapper module around a single array \n",
    "            # of numbers - around a single tensor, \n",
    "            # but it allows you to access into the rows of the weight and position\n",
    "            # embedding)\n",
    "\n",
    "            # to reflect the .h0 till .h11 layers, you can use an nn.ModuleList to index \n",
    "            # it using integers, just like the .h0 to .h11 schema in hugging faces\n",
    "            # implementation\n",
    "\n",
    "            # Moreover, gpt2 paper added another normalization layer, which\n",
    "            # is reflected here with ln_f\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # called output embeddings in attention is all you paper\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # called positional encodings in attention is all you paper\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # h block is the transformer blocks in attention is all you paper\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # ln_f is added in the arrow after the transformer architecture in attention is all you paper\n",
    "        ))\n",
    "\n",
    "        # gpt2 openai paper also used a final classifier - the llm head,\n",
    "        # which projects from 768 embedding dimensions all the way to the vocab\n",
    "        # or token size, which is 50k+\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False) # is the linear part in attention is all you paper\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # index is of shape (B, T) (batch dimension of B and time dimension of up to T, and T cannot be more than block size)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T), using arange which is a range function for pytorch.\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "\n",
    "        # addition operation of both, which has broadcasting hidden within it (position embeddings are going to be identical for every single row of input, so broadcasting is natural)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer!\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size), which is the tensor that we obtain\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    # load the weights from huggingface\n",
    "    # from_pretrained is a constructor (or class method) in python that returns the GPT object if we just give it the model_type\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        # Loads pretrained GPT-2 model weights from huggingface\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # create config object and then add the model parameters\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "\n",
    "        # set the hyperparameters to our 124M parameter model (50257, 1024)\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        # build our own model from scratch\n",
    "        model = GPT(config)\n",
    "\n",
    "        # create state dict both for our model and the hugging face gpt2 model\n",
    "        sd = model.state_dict()\n",
    "        \n",
    "        # get the keys from the huggingface gpt2 model and copy over those tensors \n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "# autodetect the device \n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "num_return_sequences = 1\n",
    "max_length = 200\n",
    "temperature = 0.7\n",
    "\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "# it is a good practice to put the model on eval mode when you're not training it\n",
    "model.eval()\n",
    "# move all the computation to a gpu cluster/singular high processing gpu (I rented an H100 from LambdaLabs!, created an instance, and connected this to cursor)\n",
    "model.to(device)\n",
    "\n",
    "# prefix tokens\n",
    "import tiktoken\n",
    "# get gpt2 encoding (tokenizer for gpt2) and encode the string \"Hello, I'm a large language model\"\n",
    "\n",
    "story_prompt = \"Once upon a time in a small village nestled between ancient mountains, there lived a curious child who discovered a mysterious door hidden in the forest. When they opened it,\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(story_prompt)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to(device)\n",
    "\n",
    "\n",
    "# generate sequences now!\n",
    "\n",
    "# Generate the story\n",
    "# Generate the story\n",
    "with torch.no_grad():\n",
    "    for i in range(max_length):\n",
    "        # Forward pass\n",
    "        logits, _ = model(x)\n",
    "        \n",
    "        # Get the logits for the last position\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply temperature sampling (use our custom temperature)\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        \n",
    "        # Sample tokens\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append the new tokens\n",
    "        x = torch.cat([x, next_tokens], dim=1)\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Generated {i}/{max_length} tokens\")\n",
    "\n",
    "# Decode and print the story\n",
    "print(\"\\n==== GENERATED STORY ====\\n\")\n",
    "for i in range(num_return_sequences):\n",
    "    generated_story = enc.decode(x[i].tolist())\n",
    "    print(generated_story)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
